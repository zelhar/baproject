\documentclass[a4paper,10pt]{article}

%\usepackage[utf8]{inputenc} 
%\usepackage[square,sort,comma,numbers]{natbib}
%\usepackage[backend=biber,autocite=inline,style=authoryear]{biblatex}
\usepackage[backend=biber,autocite=inline]{biblatex}
\addbibresource{mybib.bib}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{color}
\usepackage{enumerate}
%\usepackage{IEEEtrantools}
%\usepackage[redeflists]{IEEEtrantools}
\usepackage{verbatim}
\usepackage{graphicx}

% Basic data
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\ASSIGNMENT}{2}
\newcommand{\B}{\{-1,1\}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Inf}{\textbf{Inf}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\NS}{\textbf{NS}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\aufgabe}[1]{\item{\bf #1}}
\newcommand{\bvec}[1]{\mathbf{#1}}
\newcommand{\bv}[1]{\mathbf{#1}}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\gt}{>}
\newcommand{\half}[1]{\frac{#1}{2}}
\newcommand{\lt}{<}
\newcommand{\tuple}[1]{\langle #1 \rangle}

\newcommand{\suftab}{\text{suftab}}

\setlength{\parskip}{1.0em}
\setlength{\parindent}{1em}

\lstset{
%basicstyle=\footnotesize,
%basicstyle=\ttfamily\footnotesize,
basicstyle=\ttfamily\small,
%basicstyle=\ttfamily\scriptsize,
frame=single,
numbers=left,
%numbersep=5pt,
numberstyle=\tiny,
showspaces=false,
showstringspaces=false,
tabsize=2,
breaklines=true,
%escapeinside={#*}{*#},
%escapeinside={*\&}{\&*},% if you want to add LaTeX within your code
%mathescape=true,
%language=C++
}


%\renewcommand{\[}{\begin{IEEEeqnarray}{l"s}}
%\renewcommand{\]}{\end{IEEEeqnarray}}
%\newcommand{\equ}{\begin{IEEEeqnarray*}{c"s}}
%\newcommand{\uqe}{\end{IEEEeqnarray*}}

\theoremstyle{definition}
\newtheorem{mydef}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
%\newtheorem{remark}{Remark}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
%\newtheorem{thm}{Theorem}[mydef]
\newtheorem{lemma}{Lemma}[section]
%\newtheorem{lemma}{Lemma}[mydef]

\begin{document}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

% Document title
\begin{titlepage}
    \title{Notes about Network Propagation}
    \author{Yiftach Kolb}
    %\author{\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
    \date{\today}
\end{titlepage}

\maketitle

% Use this to give math equations different colot, but it may be buggy.
%\everymath{\color{blue}}
%\everydisplay{\color{blue}} 

\section{Mathematical Primer}
\subsection{Matrix Algebra Primer}

\begin{mydef}
\label{def:transition}
A \textbf{Transition Matrix} is a real valued non negative square ($n^2$) matrix $A$ s.t. each of its
columns sums up to one: $$\forall j \sum_i A_{i,j} = 1,\ \forall i,j A_{i,j} \geq
1$$ 
$A$ acts from the left as a linear mapping:
$T(v) := Av, \forall v \in R^{n}$. In the following script we might
interchangeably and indistinguishably use $T$ (the mapping) or $A$ the matrix.

A transition is \textbf{positive}, designated by $A > 0$ if all its entries are positive.

A transition is \textbf{regular} if for some $k>$ $A^k$ is positive. The same
property is called \textbf{primitive} in some other sources.

A transition is \textbf{irreducible} if for every entry $A_{i,j}$ there is some $k$ such
that $A^k_{i,j} > 0$ (More on that further down).
\end{mydef}

\begin{mydef}
\label{def:state}
A \textbf{State} is a non-negative vector $v \in R^n$ s.t $\sum_i v_i = 1$.
\end{mydef}

\begin{remark}
\label{remark:state}
If $v$ is a
state then and $A$ is a transition as defined in
\ref{def:transition}, 
  $Av$ is also a state because: 
$$\sum_i(Av)_i = \sum_j v_j(\sum_k A_{j,k}) = \sum_j v_j \cdot 1 = 1$$
\end{remark}

\begin{mydef}
\label{def:abs}
Given $A \in \C^{n \times n}$ We let $|A| \in \C^{n \times n}$ be the resulting
matrix from applying $|\cdot|$ element wise. Given a vector $v \in \C^n$ we let
$|v| \in \C^n$ the corresponding non-negative vector.

We also let $A \gt 0, v \gt 0$ mean that it holds coordinate wise. 
\end{mydef}

\begin{mydef}
let $u \in \C^n$, then we define:
$\tilde{u} = (|u_i|)_{i \in [n]}$, meaning the entires of $\tilde{u}$ are the
absolute values of the entries of $u$. It follows then that both have the same
$l_k$ norms.
\end{mydef}

\begin{remark}
\label{remark:abs}
If $u \in \C^n$ is on the unit circle and $T$ a transition then
$|u|$ is a transition, meaning $\||u|\|=1$, so $T|u|$ is also a transition
so $\|T|u|\|=1$.

We have (component-wise) $|Tu| \leq T|u|$. If $T>0$ and $u$ has negative or
non-real entries, then this inequality must be strict and
then $\|Tu\| \lt \|T|u|\| = 1$.
\end{remark}

\begin{lemma}
\label{lem:exist1}
If $T$ is a transition, then
there is a state $u$ such that $Tu = u$.
\begin{proof}
Because the columns of $A$ all sum to $1$, the columns of $A-I$ all sum to $0$.
Therefore $(1,1, \dots, 1)$ is an (left) eigenvector of the rows with eigenvalue $1$.
Therefore there is also some real (right) column eigenvector with eigenvalue $1$. 
(Also it follows from the Brouer fixed point theorem because $T$ maps the $l_1$
sphere to itself).

Let $u \in R^n$ be such vector: $Au=u$. Let $u = u^+ + u^-$ such that $u^- =
\min(u_i,0)$ and the other one defined similarly for the non-negative components.

Because $A$ is non-negative $A(u^+) \geq (Au)^+ \geq 0$,
and $A(u^-) \leq (Au)^- \leq 0$.

From $A$ being
non-negative and $(Au)^+ + (Au)^- = Au = u = u^+ + u^-$
And also $(A u)^+ = (u)^+$, so we must have $Au^+ \geq (Au)^+ = u^+$ 
(component wise). But if we had a strict inequality we would get:
$\|A(u^+/\|u^+\|_1)\|_1 > 1$ which is a contradiction to $A$ being a transition
matrix.

Then $A u^+ = u^+, A u^- = u^-$ and one of them must be non-zero. It follows
that $A$ has a non-negative eigenvector with eigenvalue $1$ (one of $u^+, -u^-$
which is non-zero). If we $l_1$-normalize that eigenvector it becomes a state.
\end{proof}
\end{lemma}


\begin{lemma}
\label{lem:uniq1}
If a transition $A \gt 0$ (or primitive)
, then it has exactly
one eigenvector $v$ with eigenvalue $1$ and in addition it can be chosen so that $v > 0$
Any for any other eigenvalue
$\lambda$ it holds that $|\lambda| \lt 1$.

If $A$ is just irreducible then then again $v>0$ and is unique but there may be
additional eigenvalues on the unit circle.

\begin{proof}
Let $A \gt 0$ be a transition. We already know that there exists at least one such eigenvector.
Let $u,v$ s.t $Au=u, Av=v$. 
We can assume these are real vectors because $A$ has only real entries.
Therefore we can choose $u,v$
to be states as we have already proven above.

Then let $w=u-v$. So $Aw = A(u-v) = u-v = w$. 
And $\sum_i w_i = 1 - 1 = 0$ by choice of $u,v$.

Like before $Aw^+ = w^+, Aw^- = w^-$
and because $w \neq 0$ but sums up to $0$, both $w^+, -w^- > 0$.
Because $w^-$ is non zero exactly on entries where $w^+$ is zero and vice versa, 
each of them must have at least one $0$ entry (and one none zero). But because
$A$ is strictly positive and $w^+$ is non-negative, $Aw^+$ must have ONLY
positive entries, contradicting $Aw^+ = w^+$. It follows then that $u-v=0$ is
the only possibility, hence the eigenvector with $1$ as eigenvalue is unique.

Suppose there is $Aw = \lambda w$ where $| \lambda|=1$. Choose $w$ so it is
$l_1$-normalized. Then $|w| = |Aw| \leq A \cdot |w|$ If $w$ has any negative or
complex coordinantes, then $|Aw| \lt A|w|$ and therefore 
$1 = \| |Aw| \|_1 \lt \|A|w|\|_1 =1$, a contradiction. Therefore there cannot be
any other eigenvalues on the unit circle.

Extending this for primitive matrix is easy because for some sufficiently big
$k$ $A^k \gt 0$. 

To prove the uniqueness of the $1$-eigenvector for the irreducible case, we have
$(\forall k \in \N) A^kw^+ = w^+$ and from that with some more work left undone
it follows that $w^+ > 0$ or
$w^+ = 0$.
\qedsymbol

\end{proof}
\end{lemma}

\begin{remark}
\label{remark:rhoisone}
The lemmas and theorems in this section are phrased in term of transitions. They
hold true almost verbatim in the more general case of positive/non-negative
linear transformations.

In general a non-negative linear transformation has a spectral radius $\rho$. In
the case of positive maps there is a unique single eigenvector with $\rho$ as
the unique greatest eigenvalue and so forth \dots. Because we deal with
transition maps, they all habe spectral value of $1$ so we don't use $\rho$.
In the general case replace any mention of eigenvalue $1$ with $\rho$.
\end{remark}

\begin{comment}
So far we have seen than if $T$ is a transition it has a stationary state. If it
is positive (or primitive) this stationary state is unique and is the only
eigenvector on the unit circle (all other eigenvalues are smaller).

Now we want to find out more about the spectrum of $T$ in the positive (regular)
case. We want all the other eigenvalues to be strictly less than $1$, because
then we could easily conclude that the long term distribution always converges
to the stationary.

So in fact the Perron-Frobenius theorem guaranties (for example see
\cite{meyer2000matrix}) that the other eigenvalues
are indeed strictly less than $1$. But we can also try to show that directly
below.
\end{comment}


\begin{comment}
\begin{lemma}
\label{lemma:unit root}
Let $T$ be a transition. Let $\lambda \in \C$ be an eigenvalue on the unit
circle, $|\lambda|=1$ with eigenvector $v$. Then $\exists k \in \N \lambda ^k =
1$ (namely $\lambda$ is a unit root!).

\begin{proof}
Suppose $\lambda$ is not a unit root. Then the set $\{\lambda^k\}_{k \in \N}$ is
dense in the unit circle. Therefore there is a subsequence s.t 
$\lim_{i \to \infty} \lambda^{k_i} \to 1$. By moving to subsequences we further
restrict $k_i$ so that $T^{k_i}$ converges for all coordinates. Now define a map:
$F(x) = \lim_{i \to \infty} T^{k_i}x$. This map is itself a transition. But
$F(v) = \lim T^{k_i} v = v$. Since a transition can only have one eigenvector
with eigenvalue $1$ we reached a contradiction. It follows that $\lambda$ must
be indeed a unit root. $\qedsymbol$

\end{proof}
\end{lemma}
\end{comment}

\begin{thm}
\label{thm:transition_ev}
Let $T$ be a positive (or primitive) transition. Then 

1. $1$ is the greatest eigenvalue
of $T$ and it has one unique eigenvector which is also positive,
so there exists a unique stationary state.

2. All the other eigenvalues have absolute value strictly less than $1$.

3. For every state $u$, $T^ku$ converges to the stationary state $\pi$.
In particular the columns of $T$ converge to $\pi$.

\begin{proof}
1 and 2. We already know.

3. There is a Jordan decomposition $T = PJP^{-1}$. such that $J$ is a Jordan
matrix, $j_{1,1} = 1$ and the rest of the main diagonal $|J_{i,i}| <1$.
So now the convergence is clear $J^k \to (e_1 | 0 \dots | 0)$.
For the matrix $P$ it must hold that $P = (P_1, \dots, P_n)$ and $P_1$ is the
eigeigenvector of $T$ corresponding to $1$ which we are free to $l_1$ normalize 
and the first row of $P^{-1}$ is the
left eigeigenvector corresponding to $1$ and so force \dots.

Some more work or literature check should confirm that $T^k \to (v|v\dots|v) = V$.
Then one can verify $Vu = v$ for any state $u$.
\end{proof}
\end{thm}

\begin{thm}
\label{thm:transition_irr_ev}
Let $T$ be an irreducible positive (or primitive) transition. 
Then:

1. Then $1$ is the greatest eigenvalue
of $T$ and it has one unique eigenvector which is also positive,
so there exists a unique stationary state.

2. If there are other other eigenvalues on the unit circle then their algebaic
multiplicity is equal their geometric multiplicity.

3. For every state $u$, $\frac{1}{n}\sum_{k=1}^n T^ku$ converges to the stationary state $\pi$.
In particular the columns of $T$ converge to $\pi$.

\begin{proof}
1 We already know.

2. There is a Jordan decomposition $T = PJP^{-1}$. such that $J$ is a Jordan
matrix, $j_{1,1} = 1$ and the rest of the main diagonal $|J_{i,i}| \leq 1$.

If we had a Jordan block of size greater than $1$ for an eigenvalue $\lambda$,
Then on the superdiagonal of $J^k$ we would have $k \lambda$. If $\lambda =1$
then $J^k$ and hence $T^k$ would be unbounded, but that is impossible since
$T^k$ is a transition. If follows that all eigenvalues on the unit circle must
be simple (alebgaic multiplicity equals geometric).

3. For the convergence, it follows from calculations on the Jordan blocks, which
I omit. See \textcite{meyer2000matrix} for rigorous proof.
\end{proof}
\end{thm}

\subsection{More on Matrices, Graphs and Stochastics}

A directed graph can be uniquely represented by its adjacency matrix.
$A_{i,j} = 1$ if and only if there is a directed edge from $i$ to $j$ (if we
want to use it for transitioning on columns as done above). It's possible to
assign different edge weights rather than only $1$ and $0$. If the graph is
undirected each edge would be counted in both directions and the matrix is
symmetric.
Relabeling the vertices of a graph yiedls an adjacency matrix that is similar by
permutations ($PAP^{-1}$, where $P$ is a permutation matrix) and vice versa.


To turn $A$ into a transition, normalize each column by dividing it with
its out going rank, so let $D_{i,i} = \text{outrank}(i)^{-1}$, $AD$ is the
transition matrix of this graph (because right-multiplying by $D$ normalizes each
column by its rank).

A graph is strongly connected (meaning there is a directed path form any edge
to any edge) iff the adjacency matrix is irreducible. A matrix is 
Irreducible by definition if it is NOT
similar by permutations matrices to a block upper triangular matrix.
Equivalently, a matrix $A$ is irreducible iff for each $i,j$, there is some $m$,
such that $A^m_{i,j} \gt 0$. So regularity is a stronger condition than
irreucibility and also I think it is easy to that if $A$ is irreducible then
$I+A$ is regular, because as we exponentiate it every positive entry remains
positiv \dots. 

We first show that the two condidtions for irreducibility of a matrix are
equivalent.
If $A$ is similar to a block  upper triangular matrix, than clearly the bottom
left $0$-block is going to stay all-$0$s in the similar matrix and the
corresponding entries in the original matrix as well. 
So there are $i,j$ such that $\forall m A^m_{i,j} = 0$ 

On the other direction if $A$ has indices $i,j$ such that $\forall m A^m_{i,j} =
0$ rename $i$ to $1$ and $j$ to $n$. Now consider the graph $G$ that $A$ is its
adjacency matrix. Then rename the vertices so that then first $k$ vertices are
all the vertices reachable from $1$, and the other are the vertices unreachable
from $1$ (we know that $n$ is unreachable). Since there is no edge between
vertice from $\{1 \dots k\}$ to any vertice of $\{k+1 \dots n\}$ the adjacency
matrix is upper block-triangular $\qedsymbol$.

Now we show that if $G$ is strongly connected then $A$ is irreducible:
Let graph $G$ be strongly connected and $A$ ist adjacency matrix. 
We want to show that it is irreducible.
Let $G^n$ be the graph on the same vertices such that $i,j$ have an edge in
$G^n$ iff there is a path of length $n$ in $G$ connecting them. Then its
adjacency matrix is exactly $A^n$ converted to boolean (positive means true). It
is relatively easy to show by induction but I won't show it reigorously. Just
consider that if and only if there is a pathe $i$ to $k$ to $j$ then 
$A_{i,k}A_{k,j} > 0$. So from here we see that if $G$ is strongly connected,
then for any $i,j$ we can find some $m$ so that there is is an $m$-path
connecting them and therefore $A^m_{i,j} >0$.

Finally we show the
other direction: if the graph $G$ is not strongly connected we want to show its
adjacency matrix is reducible. We take a node $i$ that has a minimal number of
reachable nodes. If $i$ is a sink then we switch it's name to $0$. We assume by
induction that the rest of the nodes have some permutations that results in a
triangular matrix and then we trivially extend it with the sink node that we
took out as the new first column. If $i$ is not a sink by minimality there must
be a cycle of minimally connected nodes including $i$, so we reduce all of them
to one representative which must be a sink of the reduced graph. Now build the
block triangular matrix on the smaller graph (so induction hypothesis), then
extend it again which should be easy because the removed nodes form a block that
connects only to itself (columns with $1$ only between said indices, $0$
otherwise) so we can put them as the first indices of the matrix $\dots$.

It is possible to turn a reducible transition into a regular and therefore irreducible by way of
random restart as explained in the article \textcite{cowen2017network}.

If we start with the adjacency matrix and replace each $0$ with some small
$\epsilon$ representing a light weight edge, then normalize the rows, we would
get the same thing. Another way to see it: take two transition matrices $P$ and
$Q$, then any convex combination of them, namely $\alpha Q + (1-\alpha)P$ for
$\alpha \in [0,1]$ is also a transition because the rows clearly still all sum
to $1$.

\begin{remark}
\label{remark:permutations}
Regarding permutation matrices and similarity.
There is a natural isomorphism between the permutation group $S_n$ and the
parmutation matrices of $n\times n$ size: 
$\pi \mapsto (e_{\pi(1)},\dots, e_{\pi(n)})$. If $P$ is a permutation matrix
with a corresponding permutation $\pi$, 
and $A$ any ($n$ square) matrix, then $PA$ is be the matrix obtained by
permuting the rows of $A$ according to the permutation $\pi$. $AP$ is the result
of permuting the columns by the permutation $\pi^{-1}$, why is that? see below
for the explanation but if you just consider that $P = \prod \Theta_i$ is the
product of permutation matrices that each corresponds to a 2-cycle permutations
(I think these are called transpositions), it becomes clear.

Now when we are dealing with adjacency matrices, we want to rename the indices,
thereby rearranging the adjacency matrix to be block triangular. This
'rearrangement' means exactly multiplying it from left and right by some
$PAP^{-1}$, and from here arises this similarity condition. If we recall every
permutation $\pi$ is a composition of 2-cycles, and 2-cycles are their own
inverse. So when we switch indices $i$ and $j$ what we actually do is switching
row $i$ with $j$m then permuting column $i$ with $j$. Then, we permute indices
$i'$ and $j'$, so the row permutation will stack up from the left, the column
permutation will stack from the right, and thus arises this $PAP^{-1}$ type of
matrix from the original adjacency matrix.
\end{remark}

\begin{lemma}
\label{theonelemma}
If $s$ is a state (column), $T$ is a transition, and $\bar{1}$ is the matrix with all $1$'s,
then $\bar{1}s = (1,1,\dots,1)^t = \mathbf{1}$ and $\bar{1}T = \bar{1}$.
\begin{proof}
Every element of the product matrix is a sum of a column of $T$, hence $1$.
$s$ is the same it's like taking just one column of $T$.

We therefore see that, using the notations of the article
\parencite{cowen2017network}
Since $\bar{1} \cdot P(t) = \bar{1}$, using 
$(\frac{(1-\alpha)}{N}\bar{1} + \alpha W)$ as the transition matrix (where $W$
is the transition originated from the normalized adjacency matrix of the
original graph), we get:

$$
P(t+1) = (\frac{(1-\alpha)}{N}\bar{1} + \alpha W) \cdot P(t) = 
\frac{(1-\alpha)}{N}\mathbf{1} + \alpha P(t)
$$

\end{proof}
\end{lemma}

\begin{remark}
Instead of taking the uniform distribution, let $s$ be some state, and let
$S = (s,s,\dots,s)$ a matrix whose columns are all $s$. Then if $p$ is any
state, $Sp = s$. So we can replace $\mathbf{1}/N$ with $S$ in the above
remarks, so our random restart distribution can be chosen arbitrarily we
actually don't need to require that that
it is has no $0$ entries:

Let $A,B \neq 0$ be non-negative square matrices, such that $A$ has a positive
row, and $B$ is irreducible. Then for all sufficiently large $k$  $(A+B)^k \gt
0$ and therefore $A+B$ is regular. The proof is pretty straight forward. What's
important to our case is that the adjacency matrix of a connected graph has the
property of $B$ and the matrix like $S$ has the property of $A$. Therefore the
composite matrix is regular fro any arbitrary restart state $s$.

\end{remark}


\begin{remark}
As we have seen for $T$ regular, and any state $s$, 
$T^ks \to \pi$ where $\pi$ is the unique stationary state.

$\pi$ can be approximated by itetrating this sequence until sufficent accuracy 
has been reached.

There is also a direct solution for the random walk with restart which goies as
follows: let $p > 0$ be a state (no $0$ entries), let $W$ be its
column-normalized adjacency matrix, and let $P = (p,\dots,p)$. as we have seen above,
$T = (1 - \alpha)P + \alpha W$ is an irreducible transition matrix
therefore $\pi$ exists and unique.

Let $p_0 = p, p_k := Tp_{k-1} = p_0(1-\alpha) + \alpha p_{k-1}$
Then $p_k \to \pi$ and 
We have the relation:

$$
I \pi = \lim(p_k) = (1 - \alpha)p_0 + \alpha \lim(W p_{k-1}) 
= (1-\alpha)I p_0 + \alpha W \pi
$$

and rearragement gives:

$$
(I - \alpha W)\pi = (1 - \alpha) p_0
$$

Now because $W$ is a transition and $0 < \alpha < 1$, for any $v$
such that $\|v\|_1 = 1$, then $\tilde{v}$ is a state and
it holds that:

$$
\|\alpha W v\|_1 \leq \alpha \|W \tilde{v}\| 
= \alpha \lt 1
$$

This guaranties that $I - \alpha W$ is invertible and
the direct solution is:

%\renewcommand{\[}{\begin{IEEEeqnarray}{l"s}}
%\renewcommand{\]}{\end{IEEEeqnarray}}

\[
\pi = (1 - \alpha) (I - \alpha W)^{-1} p_0
\]

\end{remark}

\section{Toying around with graph propagation}
\subsection{Motivation}
We work with random graphs genertated by one of the standard models (Erdos- \dots)
because the real world graphs have similar topology.

For a given category we expect only a small number of proteins play a direct role and hence should carry
its annotation, some of them are already known. Ideally we want that with some choice of $\alpha$ (the restart factor),
We get a stationary distribution that is fairly concentrated exactly on the proteins that belong to that category.
Is it possible?

Even stronger evidence: we start with $2$ or more mutally exclusive categories and partial annonations for each,
and we want that the stationary distributions will reflect that- namely they should be concentrated on non-overlapping subsets
of graph nodes.



\section{The Markov Random Fields Methd}
We use almost the same terminology of \textcite{deng2002prediction}
so I won't repeat everything here.

We have a PPI and partial annotaion which gives the potential function $U(x)$
which actually depends on the parameter $\Theta =(\alpha, \beta, \gamma)$
And we want to maximize:

\begin{equation} \label{eq:gibbs_dist}
P(X | \Theta) = \frac{1}{Z(\Theta)}\exp(-U(x))
\end{equation}

To eliminate $Z(\Theta)$ from the equation we are looking at:

\begin{equation} \label{eq:gibbs_samp}
P(X_i=1 | X_{[-i]}, \Theta) =
\frac{\exp(\alpha + (\beta -1)M_0^{i} + (\gamma - \beta)M_1^{i})}{
1 + \exp(\alpha + (\beta -1)M_0^{i} + (\gamma - \beta)M_1^{i})}
\end{equation}

If we think of every node labeled with $1$ as being occupied by a traveling
agent, who in the next iteration is going to visit some other (i.e not allowed
to stay in the same place) node according to some
probability, then $P(X_i = 1 | X_{[-i]}, \Theta)$ is the probability 
That node $i$ will be visited in the next iteration. In the propagation model
we look at the neighbours of $i$ that have a visitor in them, and we 
look for each of these how many other neighbours it has to calculate the 
visit probability of $i$, whereas in equation \eqref{eq:gibbs_samp} we don't
explicitly look at neighbours of neighbours but rather consider direct
interaction between $i$ and all its neighbours.

In the propagation model without restart, we have (of course with 
the resulting
value being capped at a maximum of $1$):

\begin{equation}
\label{eq:gibb_propagation}
P(X_i=1 | X_{[-i]}) = 
\sum_{j \in \text{Nei}(i)} \frac{1_{X_J}}{\text{Nei}(j)}
\end{equation}

Here we assume that the visiting agent will choose one of the neighbours at
equal probability. We can also formulate this similarly to the random restart
case. Also perhaps it would make sense, to consider instead of $X_{[-i]}$, a
label assignment of all the nodes including $i$, then update $X_i$ in the next
iteration by the probability that it would be visited in the next iteration.
And in this case we can add restart an reformulate \eqref{eq:gibb_propagation}
as: 

\begin{equation}
\label{eq:gibb_propagation_restart}
P(X_i=1 | X) = 
(1-\alpha) \sum_{j \in \text{Nei}(i)} \frac{1_{X_J}}{\text{Nei}(j)}
+ \alpha \sum_{j : X_j=1} \frac{1}{\text{Nei}(j)}
\end{equation}

$P(X_i=1 | X)$ here means that giving a labeling $X$ to all the nodes, we
calculate the probabiliyt that $X_i$ will be labeled $1$ in the next iteration
which is the probabilty that at least one of the agents will choose to visit it.

%I believe if we use gibbs sampling based algorithm with \eqref{eq:gibb_propagation_restart}
%to find the $\pi$ the stationary distribution but I am not sure if it is 
%mor efficient than the iterative method. 

\subsubsection{Estimation of the parametes}
The authors of \textcite{deng2002prediction} used the quasi-likelihood
approach and logistic regression.

If I understand this correctly, they take the subnetwork of annotated proteins
(of a specific function), that gives a binary vector $X = (X_i)_{i=1 \dots m}$
and each $X_i$ is treated as an observation that is independent from the other
observations, then they find the paramers that best fit the logistic model to
the sample distribution.

%I have been doing some reading in \textcite{goodfellow2016deep} (chapters
%16,17,18) to
%get more understanding on the topic because this looks like
%it could be worth trying a machine learning algorithm ob \eqref{eq:gibbs_dist} 
%instead of logistic regression.

\subsubsection{Estimating the probabvilty}
In the article, for a given a function annotaion, 
$\pi$ is the probability that a protein has that  
a protein has that function annotation (disregarding the information
from the PPI network). This $\pi$ is used to assign random values to the missing
data.

\textbf{An Idea}: Maybe we can use use the pagerank (and a cutoff) for the
initial assignment instead?

$\qedsymbol$

%It seems like a "no brainer" since we are fundementally assuming the PPI
%structure carries significant information about the functionality.

%\subsubsection{Temporary Conclusion}
%If I have to guess how Propagation network will compare to Gibbs sampling I am
%going to guess the latter will win, and the formaer is too naive.
%
%However I think there is potential room for improvement by using 
%machine learning methods to estimate $\Theta$ instead of linear logistic,
%mybe using pagerank to estimate
%$\pi$.


\nocite{herstein_winter_1989}
\nocite{meyer2000matrix}
\nocite{lawler2010random}
\nocite{girvan2002community}
\nocite{newman2006modularity}
\nocite{slides_from_lecture}

\section{testing}
\subsection{citations}

For more info see (textcite) \textcite[631]{meyer2000matrix}.

This is a normal (cite)~\cite[p.~115]{big}.

This is an (autocite) see \autocite[231]{lion2010}.

Change autocite in the usepackage definition to suit your needs. Use textcite if
you want to insert inline cite that includes the author's name.

Here is a \cite{WikipediaPerronFrobenius} cite from wikipedia.

Just cite is just the normal citation command and nocite will make sure that the
reference appears in the bibliography even though it may have not specifically
been mentioned in the text somewhere.

\subsection{Images}

Here is an image

%\includegraphics{BA20-1.png}

\begin{figure}[!htb]
  \centering
    \includegraphics[width=0.65\linewidth]{Heatmap BA20-1 alpha=0.10.png}
  \caption{Heatmap. Alpha=0.1}
  \label{fig:heatmap}
\end{figure}

Refer to it with fig \ref{fig:heatmap} tada.


\begin{figure}[!htb]
  \centering
    \includegraphics[width=0.65\linewidth]{test2.png}
  \caption{The apparatus used in the experiment. The forward right
  wing is fixed to the chain wheel which controls its elevation.
  The electrodes sticking out of the black precision clamp 
  are positioned to touch the exposed meso N1 at the root of the
  wing}
  \label{fig:apparatus}
\end{figure}

\subsection{Tables}

\begin{table}[!htb]
  \caption{Nernstsche Gleichgewichtspotenziale und das resultierende
  Membranpotenzial nach Goldmann Gleichung} 
\begin{center}
  \begin{tabular}{ | l | c | c | c | c | }
    \hline
     Ion & rel' Permeabilität &  Konz. in & Konz. auß & GG
     Potenzial \\ \hline
     $K^+$ & 1 & 124 & 4 & -86.74 \\
     $Na^+$ & 0.04 & 50 & 470 & 56.6 \\
     $Cl^-$ & 0.3 & 55 & 580 & -59.51 \\
    \hline
    $V_m$ &
    \multicolumn{4}{ | c |}{-51.35} \\
    \hline
  \end{tabular}
  \label{table:amp/dauer}
\end{center}
\end{table}

\begin{comment}
\section{Tests}
\nocite{herstein_winter_1989}
\nocite{goodfellow2016deep}
%See \citetext{big} and \nocite{herstein_winter_1989}
%\citet*{meyer2000matrix}
\nocite{meyer2000matrix}


%I doubt that there is any useful information here.~\cite{wikibook}
%
%All we know is limited, apart from knowing the answer we all
%know. Or do we? \citeauthor{wombat2016} have discovered some interesting
%things.~\autocite[compare][80]{wombat2016}
%
%What can happen to nosy people is described by \textcite[9]{lion2010}.
%
%Also \parencite{lion2010} for parentices cite \dots
%and \autocite{wikibook}

\end{comment}

\section{Reference}
%\begin{itemize}
%\item{} Herstein: Matrix Theory and Linear Algebra
%\item{} Meyer: Matrix Analysis and applied linear Algebra
%\item{} Cowen et al. slides and aritcle
%\end{itemize}

%\bibliographystyle{plainnat}
%\bibliography{mybib} 
\printbibliography
\end{document}


